{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc2501d",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek47.do.txt  -->\n",
    "<!-- dom:TITLE: Exercise week 47-48 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae5111",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercise week 47-48\n",
    "**November 17-28, 2025**\n",
    "\n",
    "Date: **Deadline is Friday November 28 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef837a4",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Overarching aims of the exercises this week\n",
    "\n",
    "The exercise set this week is meant as a summary of many of the\n",
    "central elements in various machine learning algorithms we have discussed throught the semester. You don't need to answer all questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ef66b",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Linear and logistic regression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c9231",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "Which of the following is not an assumption of ordinary least squares linear regression?\n",
    "\n",
    "* There is a linearity between predictors/features and target/outout\n",
    "\n",
    " * The inputs/features distributed according to a normal/gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d319988",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "\n",
    "Not an assumption: The inputs/features are distributed according to a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acef906",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "The mean squared error cost function for linear regression is convex in the parameters, guaranteeing a unique global minimum. True or False? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1bd9aa",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "True, because the MSE cost function is quadratic in the parameters and therefore convex, ensuring a unique global minimum when $\\boldsymbol{X}^T\\boldsymbol{X}$ is full rank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bf02e",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 3:\n",
    "\n",
    "Which statement about logistic regression is false?\n",
    "\n",
    "* Logistic regression is used for binary classification.\n",
    "\n",
    " * It uses the sigmoid function to map linear scores to probabilities.\n",
    "\n",
    " * It has an analytical closed-form solution.\n",
    "\n",
    " * Its log-loss (cross-entropy) is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bdc99",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\"It has an analytical closed-form solution\" is false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab306a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 4:\n",
    "\n",
    "Logistic regression produces a linear decision boundary in the input space. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ecbe9",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "True, because the sigmoid is applied to a linear function of the inputs, so the point where the model predicts class 1 (probability = 0.5) forms a straight line or plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695e6bb",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 5:\n",
    "\n",
    "Give two reasons why logistic regression is preferred over linear regression for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f3b8c",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "**1)** The outputs in logistic regression are between 0 and 1 (probabilites), while you can get impossible probabilites in linear regression (negative or larger than 1)\n",
    "\n",
    "\n",
    "**2)** Logistic regression is designed for classification, using a proper loss function (cross-entropy), while linear regression assumes continuous targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c398642",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fac35",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 6:\n",
    "\n",
    "Which statement is not true for fully-connected neural networks?\n",
    "\n",
    "* Without nonlinear activation functions they reduce to a single linear model.\n",
    "\n",
    " * Training relies on backpropagation using the chain rule.\n",
    "\n",
    " * A single hidden layer can approximate any continuous function on a compact set.\n",
    "\n",
    " * The loss surface of a deep neural network is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d49699",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "False: The loss surface of a deep neural network is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed2727",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 7:\n",
    "\n",
    "Using sigmoid activations in many layers of a deep neural network can cause vanishing gradients. True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11241872",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "True, because the sigmoid squeezes values into a small range where its derivative is very small, so gradients shrink as they pass through many layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1865d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 8:\n",
    "\n",
    "Describe the vanishing gradient problem: Why does it occur? Mention one technique to mitigate it and explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c19243",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "The vanishing gradient problem occurs when gradients become extremely small as they are backpropagated through many layers, especially with activations like sigmoid or tanh, causing early layers to learn very slowly.\n",
    "A common technique to mitigate it is using ReLU activations, which do not squash values into a small range and therefore keep gradients larger and easier to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ad1a8",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 9:\n",
    "\n",
    "Consider a fully-connected network with layer sizes $n_0$ (the input\n",
    "layer) ,$n_1$ (first hidden layer), $\\dots, n_L$, where $n_L$ is the\n",
    "output layer. Derive a general formula for the total number of\n",
    "trainable parameters (weights + biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ddad09",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The total number of trainable parameters is:\n",
    "\n",
    "$$\n",
    "\\sum_{l=1}^{L} \\left( n_{l-1} \\cdot n_l + n_l \\right)\n",
    "$$\n",
    "\n",
    "because each layer $l$ has $n_{l-1} \\cdot n_l$ weights and $n_l$ biases.  \n",
    "Equivalently:\n",
    "\n",
    "$$\n",
    "\\sum_{l=1}^{L} n_l (n_{l-1} + 1).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2ed47",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54a83",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 10:\n",
    "\n",
    "Which of the following is not a typical property or advantage of CNNs?\n",
    "\n",
    "* Local receptive fields\n",
    "\n",
    " * Weight sharing\n",
    "\n",
    " * More parameters than fully-connected layers\n",
    "\n",
    " * Pooling layers offering some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529751d",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The statement “More parameters than fully-connected layers” is not a typical property of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefcc46",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 11:\n",
    "\n",
    "Using zero-padding in convolutional layers can preserve the input\n",
    "spatial dimensions when using a $3 \\times 3$ kernel/filter, stride 1,\n",
    "and padding $P = 1$. True or False?a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d77757",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "True, because padding by 1 on each side with a $3\\times3$ filter and stride 1 keeps the output the same height and width as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b6806",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 12:\n",
    "\n",
    "Given input width $W$, kernel size $K$, stride S, and padding P,\n",
    "derive the formula for the output width $W_{\\text{out}} = \\frac{W - K+ 2P}{S} + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a4188",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "The output width is found by counting how many times the kernel fits when sliding across the padded input.\n",
    "\n",
    "Padding expands the input width to:\n",
    "\n",
    "$$W + 2P$$\n",
    "\n",
    "Each kernel application uses \\(K\\) units of width, and the kernel moves by stride \\(S\\).  \n",
    "So the number of valid positions is:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\frac{(W + 2P) - K}{S} + 1\n",
    "$$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\frac{W - K + 2P}{S} + 1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629397f",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 13:\n",
    "\n",
    "A convolutional layer has: $C_{\\text{in}}$ input channels,\n",
    "$C_{\\text{out}}$ output channels (filters) and kernel size $K_h \\times\n",
    "K_w$. Compute the number of trainable parameters including biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540172bc",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "Each filter has:\n",
    "\n",
    "$$C_{\\text{in}} \\cdot K_h \\cdot K_w$$\n",
    "\n",
    "weights, and each output channel has one bias term.  \n",
    "With $C_{\\text{out}}$ filters, the total number of parameters is:\n",
    "\n",
    "$$\n",
    "C_{\\text{out}} \\left( C_{\\text{in}} K_h K_w + 1 \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087780b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd5f95",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 14:\n",
    "\n",
    "Which statement about simple  RNNs is false?\n",
    "\n",
    "* They maintain a hidden state updated each time step.\n",
    "\n",
    " * They use the same weight matrices at every time step.\n",
    "\n",
    " * They handle sequences of arbitrary length.\n",
    "\n",
    " * They eliminate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560058d",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "The false statement is: “They eliminate the vanishing gradient problem.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70bb6d",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 15:\n",
    "\n",
    "LSTMs mitigate the vanishing gradient problem by using gating mechanisms (input, forget, output gates). True or False? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ecd526",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "True, because LSTMs use gates to control how information flows and is stored, allowing gradients to pass through many time steps without shrinking too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ec77a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 16:\n",
    "\n",
    "What is Backpropagation Through Time (BPTT) and why is it required for training RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ee835",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "Backpropagation Through Time (BPTT) is the training method where an RNN is “unrolled” over all time steps, and gradients are computed across the entire sequence. It is required because RNN outputs depend on previous hidden states, so the model must backpropagate errors through time to update the shared weights correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e01d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Question 17:\n",
    "\n",
    "What does a sliding window do? And why would we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b810873",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "\n",
    "A sliding window extracts overlapping subsequences from a longer sequence or time series by moving a fixed-size window step by step.\n",
    "We use it to create training samples, capture local patterns, or make predictions based on recent history."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
